{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Houston Machine Learning Meet Up\n",
    "### Neilkunal Panchal\n",
    "#### Saturday 12th October 2019\n",
    "### Multi Agent Reinforcement Learning for Glide Hockey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Import Dependencies and Load Environments\n",
    "mlagents, numpy and pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from mlagents.envs.environment import UnityEnvironment\n",
    "from ddpg_agent import Agents\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the 'AirHockey' environment be sure to chose the folder path to where the App is build\n",
    "- Diplay information about the number of Brains, Agents, states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 2\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\tscale -> 1.0\n",
      "\t\tgravity -> 9.8100004196167\n",
      "Unity brain name: HockeyBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: , \n",
      "Unity brain name: HockeyPlayer\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: left, right\n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name = \"/Users/neil_panchal/Desktop/AirHockey.app\", worker_id=1)\n",
    "env = UnityEnvironment(file_name = 'AirHockey.app', worker_id=13)\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test resetting the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          3.38993621,   0.        ,  -0.        ,   0.        ,\n",
       "         -0.        ,   0.        , -20.        ,   1.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          2.70024681,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,  20.        ,   1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mode = True\n",
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[  0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           3.38993621   0.          -0.           0.\n",
      "  -0.           0.         -20.           1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "for observation in env_info.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n",
      "continuous\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          3.38993621,   0.        ,  -0.        ,   0.        ,\n",
       "         -0.        ,   0.        , -20.        ,   1.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          2.70024681,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ,  20.        ,   1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env_info.rewards)\n",
    "print(brain.vector_action_space_type)\n",
    "env_info.vector_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Test Envionment with Random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward this episode: -0.710000041872263\n",
      "Total reward this episode: -5.0100003546103835\n",
      "Total reward this episode: -2.6400001933798194\n",
      "Total reward this episode: -5.0100003546103835\n",
      "Total reward this episode: -3.1560002407059073\n",
      "Total reward this episode: -0.12900003232061863\n",
      "Total reward this episode: -0.09000001195818186\n",
      "Total reward this episode: -0.08600002340972424\n",
      "Total reward this episode: -0.9900000691413879\n",
      "Total reward this episode: -1.5800001258030534\n",
      "Total reward this episode: -0.06800002604722977\n",
      "Total reward this episode: -0.051000011153519154\n",
      "Total reward this episode: 0.015999987721443176\n",
      "Total reward this episode: -0.10900003090500832\n",
      "Total reward this episode: -0.10900003090500832\n",
      "Total reward this episode: -1.280000097118318\n",
      "Total reward this episode: -0.13600002694875002\n",
      "Total reward this episode: -0.640000051818788\n",
      "Total reward this episode: -0.049000026658177376\n",
      "Total reward this episode: -1.7700001318007708\n",
      "Total reward this episode: -0.08200001530349255\n",
      "Total reward this episode: -0.11200001742690802\n",
      "Total reward this episode: -0.10200001671910286\n",
      "Total reward this episode: -0.5600000387057662\n",
      "Total reward this episode: -0.10200001671910286\n",
      "Total reward this episode: -0.05000000912696123\n",
      "Total reward this episode: -4.710000324994326\n",
      "Total reward this episode: -0.00100000761449337\n",
      "Total reward this episode: -0.12700002826750278\n",
      "Total reward this episode: -0.11700002755969763\n",
      "Total reward this episode: -0.09300001803785563\n",
      "Total reward this episode: -0.01600001845508814\n",
      "Total reward this episode: -0.3000000203028321\n",
      "Total reward this episode: -5.0100003546103835\n",
      "Total reward this episode: -1.5900001116096973\n",
      "Total reward this episode: -0.10100001469254494\n",
      "Total reward this episode: -0.10300001874566078\n",
      "Total reward this episode: -0.044000016525387764\n",
      "Total reward this episode: -0.1040000207722187\n",
      "Total reward this episode: -0.7700000535696745\n",
      "Total reward this episode: -0.1110000154003501\n",
      "Total reward this episode: -5.060000357218087\n",
      "Total reward this episode: 0.019999995827674866\n",
      "Total reward this episode: -1.0560000920668244\n",
      "Total reward this episode: -0.7300000507384539\n",
      "Total reward this episode: 0.00199997890740633\n",
      "Total reward this episode: -0.0050000157207250595\n",
      "Total reward this episode: -0.11700002755969763\n",
      "Total reward this episode: -0.9980000918731093\n",
      "Total reward this episode: -3.478000267408788\n",
      "Total reward this episode: -3.0380002362653613\n",
      "Total reward this episode: -1.0200000712648034\n",
      "Total reward this episode: -0.11800002958625555\n",
      "Total reward this episode: 0.019999995827674866\n",
      "Total reward this episode: -0.12900003232061863\n",
      "Total reward this episode: -2.7400001855567098\n",
      "Total reward this episode: -2.070000145584345\n",
      "Total reward this episode: 0.03499998711049557\n",
      "Total reward this episode: -0.10600002482533455\n",
      "Total reward this episode: -1.1620000917464495\n",
      "Total reward this episode: -0.48000003956258297\n",
      "Total reward this episode: -0.10200001671910286\n",
      "Total reward this episode: 0.0319999810308218\n",
      "Total reward this episode: -2.7200001841410995\n",
      "Total reward this episode: -4.7300003338605165\n",
      "Total reward this episode: -0.11400002148002386\n",
      "Total reward this episode: -0.10900003090500832\n",
      "Total reward this episode: -1.3200001074001193\n",
      "Total reward this episode: -0.10100001469254494\n",
      "Total reward this episode: -0.14400002360343933\n",
      "Total reward this episode: -1.0000000772997737\n",
      "Total reward this episode: -1.4000000907108188\n",
      "Total reward this episode: -3.0200002202764153\n",
      "Total reward this episode: -1.380000096745789\n",
      "Total reward this episode: -0.11700002755969763\n",
      "Total reward this episode: -0.10200001671910286\n",
      "Total reward this episode: -0.041000010445714\n",
      "Total reward this episode: -0.17300002370029688\n",
      "Total reward this episode: -3.7800002666190267\n",
      "Total reward this episode: -1.0200000712648034\n",
      "Total reward this episode: 0.048999995924532413\n",
      "Total reward this episode: -0.07500002067536116\n",
      "Total reward this episode: -0.10200001671910286\n",
      "Total reward this episode: -0.026000019162893295\n",
      "Total reward this episode: -2.1200001565739512\n",
      "Total reward this episode: -1.690000118687749\n",
      "Total reward this episode: -0.11400002148002386\n",
      "Total reward this episode: -0.1080000288784504\n",
      "Total reward this episode: -0.04800002463161945\n",
      "Total reward this episode: -0.23000001534819603\n",
      "Total reward this episode: -1.9000001335516572\n",
      "Total reward this episode: -0.11200001742690802\n",
      "Total reward this episode: -0.5900000408291817\n",
      "Total reward this episode: -1.4000000907108188\n",
      "Total reward this episode: -0.1230000201612711\n",
      "Total reward this episode: -0.09000001195818186\n",
      "Total reward this episode: -0.2840000335127115\n",
      "Total reward this episode: -0.6700000390410423\n",
      "Total reward this episode: -2.500000183470547\n",
      "Total reward this episode: -5.0100003546103835\n"
     ]
    }
   ],
   "source": [
    "for episode in range(100):\n",
    "    env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        action_size = brain.vector_action_space_size\n",
    "        if brain.vector_action_space_type == 'continuous':\n",
    "            env_info = env.step(np.random.randn(len(env_info.agents), \n",
    "                                                action_size[0]))[default_brain]\n",
    "        else:\n",
    "            action = np.column_stack([np.random.randint(0, action_size[i], size=(len(env_info.agents))) for i in range(len(action_size))])\n",
    "            env_info = env.step(action)[default_brain]\n",
    "        episode_rewards += env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "    print(\"Total reward this episode: {}\".format(episode_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Train the Agents\n",
    "- Agents: is the class for the agents. This is instantiated for 2 players\n",
    "- get_actions: returns the concatenated actions for both players in the form of a Markov Game and adds noise to encourage exploration\n",
    "- maddpg: is the multi-agent-deep-deterministic policy gradient traning method\n",
    "    - hyper parameters are: number of episodes\n",
    "    - OU noise for Policy Gradient\n",
    "    - max number of time steps\n",
    "    - reward required to solve the environment\n",
    "    - test: loads the trained weights of a NN and plays this against the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain\n",
    "#reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: [2]\n",
      "States is: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   3.32697582e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  -2.71592617e+00  1.94946098e+00  1.05776620e+00  8.94069672e-08]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   3.06033611e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   2.71592617e+00  1.94946098e+00 -1.05776620e+00  8.94069672e-08]]\n",
      "(24,)\n",
      "There are 2 agents. Each observes a state with length: (24,)\n",
      "The state for the first agent looks like: [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  3.32697582e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -2.71592617e+00  1.94946098e+00  1.05776620e+00  8.94069672e-08]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "print('States is:', states)\n",
    "state_size = np.shape(states[1])\n",
    "print(state_size)\n",
    "# state_size=8\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print((device is  \"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=train_mode)[brain_name] \n",
    "# actions = get_actions(states, addNoise)           \n",
    "# env_info = env.step(actions)[brain_name] \n",
    "# np.shape(env_info.vector_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size is: 24\n",
      "action size is : 2\n",
      "on init device is: cpu\n",
      "False\n",
      "action size is:  2\n",
      "action size is:  2\n",
      "on init device is: cpu\n",
      "False\n",
      "action size is:  2\n",
      "action size is:  2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# num_agents=1\n",
    "# agent = Agents(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=0)\n",
    "# agent.action_size\n",
    "print('state size is:', state_size[0])\n",
    "print('action size is :', action_size[0])\n",
    "agent_0 = Agents(state_size=state_size[0], action_size=action_size[0], num_agents=1, random_seed=0)\n",
    "agent_1 = Agents(state_size=state_size[0], action_size=action_size[0], num_agents=1, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions(states, add_noise):\n",
    "    '''gets actions for each agent and then combines them into one array'''\n",
    "    action_0 = agent_0.act(states, add_noise)    \n",
    "    action_1 = agent_1.act(states, add_noise)   \n",
    "    return np.concatenate((action_0, action_1), axis=0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "solved = 1\n",
    "consecutiveEpisodes = 10\n",
    "printFrequency = 1\n",
    "addNoise = True\n",
    "\n",
    "def Maddpg(n_episodes=20, max_t=1000, train_mode=True):\n",
    "    \"\"\"Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int)      : maximum number of training episodes\n",
    "        max_t (int)           : maximum number of timesteps per episode\n",
    "        train_mode (bool)     : if 'True' set environment to training mode\n",
    "\n",
    "    \"\"\"\n",
    "    scores_window = deque(maxlen=consecutiveEpisodes)\n",
    "    scores_all = []\n",
    "    moving_average = []\n",
    "    best_score = -np.inf\n",
    "    best_episode = 0\n",
    "    already_solved = False    \n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]  \n",
    "        states = np.reshape(env_info.vector_observations, (1,48)) # combine states\n",
    "        agent_0.reset()\n",
    "        agent_1.reset()\n",
    "        scores = np.zeros(num_agents)\n",
    "        while True:\n",
    "            actions = get_actions(states, addNoise)           \n",
    "            env_info = env.step(actions)[brain_name] \n",
    "#             print('shape of observations is', np.shape(env_info.vector_observations))\n",
    "#             print('observations are', env_info.vector_observations)\n",
    "#             print('actions', actions)\n",
    "            next_states = np.reshape(env_info.vector_observations, (1, 48)) # combine the agent next states\n",
    "            rewards = env_info.rewards                         # get reward        \n",
    "            done = env_info.local_done                         # see if episode finished\n",
    "            agent_0.step(states, actions, rewards[0], next_states, done, 0) # agent 1 learning\n",
    "            agent_1.step(states, actions, rewards[1], next_states, done, 1) # agent 2 learning\n",
    "            scores += np.max(rewards)                          # update the score for each agent\n",
    "            states = next_states                              \n",
    "            if np.any(done):                                   # exit loop if episode finished\n",
    "                break\n",
    "\n",
    "        ep_best_score = np.max(scores)\n",
    "        scores_window.append(ep_best_score)\n",
    "        scores_all.append(ep_best_score)\n",
    "        moving_average.append(np.mean(scores_window))\n",
    "\n",
    "        # save best score                        \n",
    "        if ep_best_score > best_score:\n",
    "            best_score = ep_best_score\n",
    "            best_episode = i_episode\n",
    "        \n",
    "        # print results\n",
    "        if i_episode % printFrequency == 0:\n",
    "            print('Episodes {:0>4d}-{:0>4d}\\tMax Reward: {:.3f}\\tMoving Average: {:.3f}'.format(\n",
    "                i_episode-printFrequency, i_episode, np.max(scores_all[-printFrequency:]), moving_average[-1]))\n",
    "\n",
    "        # determine if environment is solved and keep best performing models\n",
    "        if moving_average[-1] >= solved:\n",
    "            if not already_solved:\n",
    "                print('<-- Environment solved in {:d} episodes! \\\n",
    "                \\n<-- Moving Average: {:.3f} over past {:d} episodes'.format(\n",
    "                    i_episode-consecutiveEpisodes, moving_average[-1], consecutiveEpisodes))\n",
    "                already_solved = True\n",
    "                # save weights\n",
    "                torch.save(agent_0.actor_local.state_dict(), 'checkpoint_actor_0.pth')\n",
    "                torch.save(agent_0.critic_local.state_dict(), 'checkpoint_critic_0.pth')\n",
    "                torch.save(agent_1.actor_local.state_dict(), 'checkpoint_actor_1.pth')\n",
    "                torch.save(agent_1.critic_local.state_dict(), 'checkpoint_critic_1.pth')\n",
    "            elif ep_best_score >= best_score:\n",
    "                print('<-- Best episode so far!\\\n",
    "                \\nEpisode {:0>4d}\\tMax Reward: {:.3f}\\tMoving Average: {:.3f}'.format(\n",
    "                i_episode, ep_best_score, moving_average[-1]))\n",
    "                # save weights\n",
    "                torch.save(agent_0.actor_local.state_dict(), 'checkpoint_actor_0.pth')\n",
    "                torch.save(agent_0.critic_local.state_dict(), 'checkpoint_critic_0.pth')\n",
    "                torch.save(agent_1.actor_local.state_dict(), 'checkpoint_actor_1.pth')\n",
    "                torch.save(agent_1.critic_local.state_dict(), 'checkpoint_critic_1.pth')\n",
    "            elif (i_episode-best_episode) >= 100:\n",
    "                # stop training if model stops converging\n",
    "                print('<-- Training stopped. Best score not matched or exceeded for 200 episodes')\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    return scores_all, moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 0000-0001\tMax Reward: -0.295\tMoving Average: -0.295\n",
      "Episodes 0001-0002\tMax Reward: -0.107\tMoving Average: -0.201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neil_panchal/Desktop/ddpg_agent.py:139: UserWarning: Using a target size (torch.Size([128, 2])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(Q_expected, Q_targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 0002-0003\tMax Reward: -1.290\tMoving Average: -0.564\n",
      "Episodes 0003-0004\tMax Reward: -1.715\tMoving Average: -0.852\n",
      "Episodes 0004-0005\tMax Reward: -2.708\tMoving Average: -1.223\n",
      "Episodes 0005-0006\tMax Reward: -1.808\tMoving Average: -1.321\n",
      "Episodes 0006-0007\tMax Reward: -0.526\tMoving Average: -1.207\n",
      "Episodes 0007-0008\tMax Reward: 0.012\tMoving Average: -1.055\n",
      "Episodes 0008-0009\tMax Reward: -0.054\tMoving Average: -0.943\n",
      "Episodes 0009-0010\tMax Reward: -3.570\tMoving Average: -1.206\n",
      "Episodes 0010-0011\tMax Reward: -1.049\tMoving Average: -1.282\n",
      "Episodes 0011-0012\tMax Reward: 0.020\tMoving Average: -1.269\n",
      "Episodes 0012-0013\tMax Reward: -2.544\tMoving Average: -1.394\n",
      "Episodes 0013-0014\tMax Reward: -1.724\tMoving Average: -1.395\n",
      "Episodes 0014-0015\tMax Reward: -1.601\tMoving Average: -1.284\n",
      "Episodes 0015-0016\tMax Reward: -0.033\tMoving Average: -1.107\n",
      "Episodes 0016-0017\tMax Reward: 0.021\tMoving Average: -1.052\n",
      "Episodes 0017-0018\tMax Reward: -2.378\tMoving Average: -1.291\n",
      "Episodes 0018-0019\tMax Reward: -0.094\tMoving Average: -1.295\n",
      "Episodes 0019-0020\tMax Reward: -1.702\tMoving Average: -1.108\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores, averages = Maddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSEC_EPISODES = 10\n",
    "PRINT_EVERY = 1\n",
    "addNoise = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on init device is: cpu\n",
      "False\n",
      "action size is:  2\n",
      "action size is:  2\n",
      "on init device is: cpu\n",
      "False\n",
      "action size is:  2\n",
      "action size is:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reinitialize the agents (if needed)\n",
    "agent_0 = Agents(state_size[0], action_size[0], num_agents=1, random_seed=0)\n",
    "agent_1 = Agents(state_size[0], action_size[0], num_agents=1, random_seed=0)\n",
    "\n",
    "# load the weights from file\n",
    "agent_0_weights = 'checkpoint_actor_0.pth'\n",
    "agent_1_weights = 'checkpoint_actor_1.pth'\n",
    "agent_0.actor_local.load_state_dict(torch.load(agent_0_weights))\n",
    "agent_1.actor_local.load_state_dict(torch.load(agent_1_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 0000-0001\tMax Reward: -0.155\tMoving Average: -0.155\n",
      "Episodes 0001-0002\tMax Reward: -0.119\tMoving Average: -0.137\n",
      "Episodes 0002-0003\tMax Reward: -5.010\tMoving Average: -1.761\n",
      "Episodes 0003-0004\tMax Reward: -5.010\tMoving Average: -2.574\n",
      "Episodes 0004-0005\tMax Reward: -5.010\tMoving Average: -3.061\n",
      "Episodes 0005-0006\tMax Reward: -5.010\tMoving Average: -3.386\n",
      "Episodes 0006-0007\tMax Reward: -5.010\tMoving Average: -3.618\n",
      "Episodes 0007-0008\tMax Reward: -5.010\tMoving Average: -3.792\n",
      "Episodes 0008-0009\tMax Reward: -5.010\tMoving Average: -3.927\n",
      "Episodes 0009-0010\tMax Reward: -2.082\tMoving Average: -3.743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.1550000263378024,\n",
       "  -0.11900003161281347,\n",
       "  -5.0100003546103835,\n",
       "  -5.0100003546103835,\n",
       "  -5.0100003546103835,\n",
       "  -5.0100003546103835,\n",
       "  -5.0100003546103835,\n",
       "  -5.0100003546103835,\n",
       "  -5.0100003546103835,\n",
       "  -2.082000156864524],\n",
       " [-0.1550000263378024,\n",
       "  -0.13700002897530794,\n",
       "  -1.7613334708536665,\n",
       "  -2.5735001917928457,\n",
       "  -3.0608002243563535,\n",
       "  -3.385666912732025,\n",
       "  -3.6177145472860763,\n",
       "  -3.7917502732016146,\n",
       "  -3.9271113933581443,\n",
       "  -3.7426002697087823])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(n_episodes=10, max_t=1000, train_mode=False):\n",
    "\n",
    "    scores_window = deque(maxlen=consecutiveEpisodes)\n",
    "    scores_all = []\n",
    "    moving_average = []  \n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]         # reset the environment\n",
    "        states = np.reshape(env_info.vector_observations, (1,48)) # get states and combine them\n",
    "        scores = np.zeros(num_agents)\n",
    "        while True:\n",
    "            actions = get_actions(states, addNoise)           # choose agent actions and combine them\n",
    "            env_info = env.step(actions)[brain_name]           # send both agents' actions together to the environment\n",
    "            next_states = np.reshape(env_info.vector_observations, (1, 48)) # combine the agent next states\n",
    "            rewards = env_info.rewards                         # get reward\n",
    "            done = env_info.local_done                         # see if episode finished\n",
    "            scores += np.max(rewards)                          # update the score for each agent\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(done):                                   # exit loop if episode finished\n",
    "                break\n",
    "\n",
    "        ep_best_score = np.max(scores)\n",
    "        scores_window.append(ep_best_score)\n",
    "        scores_all.append(ep_best_score)\n",
    "        moving_average.append(np.mean(scores_window))\n",
    "\n",
    "        # print results\n",
    "        if i_episode % printFrequency == 0:\n",
    "            print('Episodes {:0>4d}-{:0>4d}\\tMax Reward: {:.3f}\\tMoving Average: {:.3f}'.format(\n",
    "                i_episode-printFrequency, i_episode, np.max(scores_all[-PRINT_EVERY:]), moving_average[-1]))\n",
    "            \n",
    "    return scores_all, moving_average\n",
    "test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
